{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDu51h2iQ9hr"
      },
      "source": [
        "# Weather Data Preprocessing: Create Weighted Features (One-Time)\n",
        "\n",
        "**Purpose:** Convert 8 regional weather CSV files into 1 final weighted file.\n",
        "\n",
        "**Input:**\n",
        "- `combined.csv` (your ERCOT load data)\n",
        "- `weather_files.zip` (8 regional weather CSV files)\n",
        "\n",
        "**Output:**\n",
        "- `final_weighted_weather_features.csv` (clean, ready for modeling)\n",
        "\n",
        "**Run this once, then use the output file in your training notebook!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffeyMUD_Q9ht",
        "outputId": "b10ac1ae-62b6-4698-d5a3-ec363c5f77b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "WEATHER DATA PREPROCESSING: CREATE WEIGHTED FEATURES\n",
            "================================================================================\n",
            "\n",
            "This notebook:\n",
            "  1. Loads combined.csv (ERCOT load data)\n",
            "  2. Unzips weather_files.zip (8 regional weather CSVs)\n",
            "  3. Calculates weights based on load contribution\n",
            "  4. Creates weighted weather features\n",
            "  5. Saves final_weighted_weather_features.csv\n",
            "\n",
            "After this, use the output file in your training notebook!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import io\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"WEATHER DATA PREPROCESSING: CREATE WEIGHTED FEATURES\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nThis notebook:\")\n",
        "print(\"  1. Loads combined.csv (ERCOT load data)\")\n",
        "print(\"  2. Unzips weather_files.zip (8 regional weather CSVs)\")\n",
        "print(\"  3. Calculates weights based on load contribution\")\n",
        "print(\"  4. Creates weighted weather features\")\n",
        "print(\"  5. Saves final_weighted_weather_features.csv\")\n",
        "print(\"\\nAfter this, use the output file in your training notebook!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxWbyh5OQ9hu"
      },
      "source": [
        "## Step 1: Load ERCOT Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXujMI3wQ9hu",
        "outputId": "51a59e17-9723-4333-8981-cd9438d74ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ Drive mounted successfully\n",
            "\n",
            "Loading load data from: /content/drive/MyDrive/combined.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1351242258.py:20: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  load_df = pd.read_csv(LOAD_FILE)\n",
            "/tmp/ipython-input-1351242258.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded: 96,430 rows × 17 columns\n",
            "\n",
            "Columns: ['Hour Ending', 'COAST', 'EAST', 'FWEST', 'NORTH', 'NCENT', 'SOUTH', 'SCENT', 'WEST', 'FAR_WEST', 'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'TOTAL', 'hour', 'day', 'year']\n",
            "\n",
            "First few rows:\n",
            "      Hour Ending        COAST         EAST        FWEST        NORTH  \\\n",
            "0  1/1/14 1:00 AM  9101.691219  1338.197939  1338.197939  1338.197939   \n",
            "1  1/1/14 2:00 AM  8907.975782  1328.940064  1809.180861   791.141630   \n",
            "2  1/1/14 3:00 AM  8738.460973  1317.990846  1804.524573   789.791906   \n",
            "3  1/1/14 4:00 AM  8622.671259  1325.545134  1807.227126   797.195877   \n",
            "4  1/1/14 5:00 AM  8615.480468  1348.596187  1816.456043   810.868332   \n",
            "\n",
            "          NCENT        SOUTH        SCENT         WEST     FAR_WEST  \\\n",
            "0   1338.197939  1338.197939  1338.197939  1338.197939  1338.197939   \n",
            "1  12297.109820  3246.493375  6091.018850  1098.774771  1809.180861   \n",
            "2  12285.295250  3217.721388  6060.017515  1097.716762  1804.524573   \n",
            "3  12368.070070  3165.203545  6017.610069  1105.891248  1807.227126   \n",
            "4  12639.017360  3134.199330  6072.948097  1123.927314  1816.456043   \n",
            "\n",
            "        NORTH_C     SOUTHERN      SOUTH_C         TOTAL hour       day  year  \n",
            "0   1338.197939  1338.197939  1338.197939   1338.197939    1  1/1/2014  2014  \n",
            "1  12297.109820  3246.493375  6091.018850  35570.640000    2  1/1/2014  2014  \n",
            "2  12285.295250  3217.721388  6060.017515  35311.520000    3  1/1/2014  2014  \n",
            "3  12368.070070  3165.203545  6017.610069  35209.414330    4  1/1/2014  2014  \n",
            "4  12639.017360  3134.199330  6072.948097  35561.493130    5  1/1/2014  2014  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/05/2017 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/04/2018 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/03/2019 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/01/2020 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/07/2021 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/06/2022 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/05/2023 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
            "/tmp/ipython-input-1351242258.py:29: FutureWarning: Parsed string \"11/03/2024 02:00 DST\" included an un-recognized timezone \"DST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Defining placeholder station keys (will be used to find files in zip)\n",
            "✓ Station keys: ['NCENT', 'COAST', 'SCENT', 'SOUTH_C', 'FAR_WEST', 'WEST', 'SOUTHERN', 'EAST']\n",
            "\n",
            "Unzipping and reading from: /content/drive/MyDrive/weather_files.zip\n",
            "✓ Found 9 files in zip archive\n",
            "\n",
            "Loading: weather_data/weather_data_NCENT.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Loading: weather_data/weather_data_COAST.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Loading: weather_data/weather_data_SCENT.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Loading: weather_data/weather_data_SOUTH_C.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Loading: weather_data/weather_data_FAR_WEST.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Loading: weather_data/weather_data_WEST.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Loading: weather_data/weather_data_SOUTHERN.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Loading: weather_data/weather_data_EAST.csv\n",
            "  'date' col not found, using first col as datetime.\n",
            "  Converting datetime to date (this truncates hours!)\n",
            "   ✓ 96432 rows\n",
            "     Columns: ['time', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature']...\n",
            "     Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "✓ Loaded 8 weather station files into `weather_files` dictionary\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# --- Step 1: Mount your Google Drive ---\n",
        "# This will prompt you to authorize the connection\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Drive mounted successfully\")\n",
        "\n",
        "# --- Step 2: Update the file path to point to your Drive ---\n",
        "# This assumes 'combined.csv' is in the main 'My Drive' folder\n",
        "LOAD_FILE = '/content/drive/MyDrive/combined.csv'\n",
        "\n",
        "# You can change this path if it's in a subfolder, e.g.:\n",
        "# LOAD_FILE = '/content/drive/MyDrive/My_Project_Folder/combined.csv'\n",
        "\n",
        "print(f\"\\nLoading load data from: {LOAD_FILE}\")\n",
        "load_df = pd.read_csv(LOAD_FILE)\n",
        "\n",
        "print(f\"✓ Loaded: {load_df.shape[0]:,} rows × {load_df.shape[1]} columns\")\n",
        "print(f\"\\nColumns: {load_df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(load_df.head())\n",
        "\n",
        "# Ensure we have a datetime column\n",
        "if 'Hour Ending' in load_df.columns:\n",
        "    load_df['datetime'] = pd.to_datetime(load_df['Hour Ending'])\n",
        "    load_df['date'] = load_df['datetime'].dt.date\n",
        "else:\n",
        "    # Try to find a datetime column\n",
        "    date_cols = [c for c in load_df.columns if 'date' in c.lower() or 'time' in c.lower() or 'hour' in c.lower()]\n",
        "    if date_cols:\n",
        "        load_df['datetime'] = pd.to_datetime(load_df[date_cols[0]])\n",
        "        load_df['date'] = load_df['datetime'].dt.date\n",
        "    else:\n",
        "        raise ValueError(\"Could not find date/time column in load data\")\n",
        "\n",
        "print(f\"\\nDate range: {load_df['date'].min()} to {load_df['date'].max()}\")\n",
        "\n",
        "# --- ADDED SECTION ---\n",
        "\n",
        "# Placeholder for weights_normalized.\n",
        "# This is needed so the .keys() loop below works.\n",
        "# You will replace this with your actual weight calculation logic.\n",
        "print(\"\\nDefining placeholder station keys (will be used to find files in zip)\")\n",
        "weights_normalized = {\n",
        "    \"NCENT\": 0, \"COAST\": 0, \"SCENT\": 0, \"SOUTH_C\": 0,\n",
        "    \"FAR_WEST\": 0, \"WEST\": 0, \"SOUTHERN\": 0, \"EAST\": 0\n",
        "}\n",
        "print(f\"✓ Station keys: {list(weights_normalized.keys())}\")\n",
        "\n",
        "\n",
        "# --- Step 3: Load Weather Data from Zip in Drive ---\n",
        "# Point this to the location of your zip file on Google Drive\n",
        "WEATHER_ZIP = '/content/drive/MyDrive/weather_files.zip'\n",
        "\n",
        "print(f\"\\nUnzipping and reading from: {WEATHER_ZIP}\")\n",
        "\n",
        "weather_files = {}\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(WEATHER_ZIP, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        print(f\"✓ Found {len(file_list)} files in zip archive\")\n",
        "\n",
        "        # Load each weather file based on the keys from our list\n",
        "        for station in weights_normalized.keys():\n",
        "            filename = f'weather_data_{station}.csv'\n",
        "\n",
        "            # Try to find the file in the zip (handles nested folders, e.g. \"weather_data/weather_data_COAST.csv\")\n",
        "            matching_files = [f for f in file_list if filename.lower() in f.lower()]\n",
        "\n",
        "            if matching_files:\n",
        "                actual_filename = matching_files[0]\n",
        "                print(f\"\\nLoading: {actual_filename}\")\n",
        "\n",
        "                with zip_ref.open(actual_filename) as file:\n",
        "                    # Use io.TextIOWrapper for correct text decoding from bytes\n",
        "                    df = pd.read_csv(io.TextIOWrapper(file))\n",
        "\n",
        "                    # Ensure date column (this logic assumes 'time' is the first col or 'date' exists)\n",
        "                    if 'date' not in df.columns:\n",
        "                        # Assuming the first column is the datetime index\n",
        "                        print(\"  'date' col not found, using first col as datetime.\")\n",
        "                        df['datetime'] = pd.to_datetime(df.iloc[:, 0])\n",
        "                    else:\n",
        "                        df['datetime'] = pd.to_datetime(df['date'])\n",
        "\n",
        "                    # ---\n",
        "                    # WARNING: The line below converts your hourly data to daily dates.\n",
        "                    # You may want to change this to keep the hourly 'datetime'\n",
        "                    # if your model is hourly.\n",
        "                    # ---\n",
        "                    print(\"  Converting datetime to date (this truncates hours!)\")\n",
        "                    df['date'] = df['datetime'].dt.date\n",
        "                    weather_files[station] = df\n",
        "\n",
        "                    print(f\"   ✓ {len(df)} rows\")\n",
        "                    print(f\"     Columns: {df.columns.tolist()[:5]}...\")\n",
        "                    print(f\"     Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "            else:\n",
        "                print(f\"\\n⚠️  {filename} not found in zip!\")\n",
        "                print(f\"   Available files (sample): {[f for f in file_list if 'weather' in f][:10]}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ {WEATHER_ZIP} not found!\")\n",
        "    print(f\"   Make sure weather_files.zip is in your Google Drive at: /content/drive/MyDrive/weather_files.zip\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\n✓ Loaded {len(weather_files)} weather station files into `weather_files` dictionary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J50xSy_PQ9hv"
      },
      "source": [
        "## Step 2: Calculate Weights from Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUYBnp-wQ9hv",
        "outputId": "321ce487-525d-4ffe-b36f-84b1c65e2cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CALCULATING WEIGHTS FROM LOAD DATA\n",
            "================================================================================\n",
            "\n",
            "Total average load: 44,246 MW\n",
            "\n",
            "Weight calculation (Region avg load / Total avg load):\n",
            "\n",
            "  Dallas          → NORTH, NCENT, NORTH_C         \n",
            "    Avg load:     27,109 MW\n",
            "    Weight:        61.3%\n",
            "\n",
            "  San_Antonio     → SOUTH, SOUTH_C                \n",
            "    Avg load:     10,455 MW\n",
            "    Weight:        23.6%\n",
            "\n",
            "  Abilene         → WEST, FWEST                   \n",
            "    Avg load:      4,633 MW\n",
            "    Weight:        10.5%\n",
            "\n",
            "  COAST           → COAST                         \n",
            "    Avg load:     12,485 MW\n",
            "    Weight:        28.2%\n",
            "\n",
            "  EAST            → EAST                          \n",
            "    Avg load:      1,548 MW\n",
            "    Weight:         3.5%\n",
            "\n",
            "  FAR_WEST        → FAR_WEST                      \n",
            "    Avg load:      3,604 MW\n",
            "    Weight:         8.1%\n",
            "\n",
            "  SCENT           → SCENT                         \n",
            "    Avg load:      6,558 MW\n",
            "    Weight:        14.8%\n",
            "\n",
            "  SOUTHERN        → SOUTHERN                      \n",
            "    Avg load:      3,616 MW\n",
            "    Weight:         8.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "NORMALIZED WEIGHTS (sum = 1.0000):\n",
            "--------------------------------------------------------------------------------\n",
            "  Dallas           38.7%\n",
            "  COAST            17.8%\n",
            "  San_Antonio      14.9%\n",
            "  SCENT             9.4%\n",
            "  Abilene           6.6%\n",
            "  SOUTHERN          5.2%\n",
            "  FAR_WEST          5.1%\n",
            "  EAST              2.2%\n",
            "\n",
            "✓ Weights calculated\n"
          ]
        }
      ],
      "source": [
        "# Define regional groupings\n",
        "region_mappings = {\n",
        "    'Dallas': ['NORTH', 'NCENT', 'NORTH_C'],\n",
        "    'San_Antonio': ['SOUTH', 'SOUTH_C'],\n",
        "    'Abilene': ['WEST', 'FWEST'],\n",
        "    'COAST': ['COAST'],\n",
        "    'EAST': ['EAST'],\n",
        "    'FAR_WEST': ['FAR_WEST'],\n",
        "    'SCENT': ['SCENT'],\n",
        "    'SOUTHERN': ['SOUTHERN'],\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CALCULATING WEIGHTS FROM LOAD DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "weights = {}\n",
        "total_avg_load = load_df['TOTAL'].mean()\n",
        "\n",
        "print(f\"\\nTotal average load: {total_avg_load:,.0f} MW\")\n",
        "print(f\"\\nWeight calculation (Region avg load / Total avg load):\")\n",
        "\n",
        "for station, regions in region_mappings.items():\n",
        "    # Check which regions exist in our data\n",
        "    available_regions = [r for r in regions if r in load_df.columns]\n",
        "\n",
        "    if available_regions:\n",
        "        # Calculate average load from these regions\n",
        "        regional_load = load_df[available_regions].sum(axis=1).mean()\n",
        "        weight = regional_load / total_avg_load\n",
        "        weights[station] = weight\n",
        "\n",
        "        print(f\"\\n  {station:15} → {', '.join(available_regions):30}\")\n",
        "        print(f\"    Avg load: {regional_load:>10,.0f} MW\")\n",
        "        print(f\"    Weight:   {weight:>10.1%}\")\n",
        "    else:\n",
        "        print(f\"\\n  {station:15} → No regions found!\")\n",
        "\n",
        "# Normalize weights to sum to 1.0\n",
        "total_weight = sum(weights.values())\n",
        "weights_normalized = {k: v / total_weight for k, v in weights.items()}\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(f\"NORMALIZED WEIGHTS (sum = {sum(weights_normalized.values()):.4f}):\")\n",
        "print(\"-\"*80)\n",
        "for station in sorted(weights_normalized.keys(), key=lambda x: weights_normalized[x], reverse=True):\n",
        "    weight = weights_normalized[station]\n",
        "    print(f\"  {station:15} {weight:6.1%}\")\n",
        "\n",
        "print(f\"\\n✓ Weights calculated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbhIDipvQ9hw"
      },
      "source": [
        "## Step 3: Create Weighted Weather Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlfy9O1FQ9hw",
        "outputId": "e1f6dbdc-319f-4f5e-acec-3c65f2a66c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CREATING WEIGHTED WEATHER FEATURES\n",
            "================================================================================\n",
            "\n",
            "Base dataframe: 4018 unique dates\n",
            "Date range: 2014-01-01 to 2024-12-31\n",
            "\n",
            "Creating weighted features:\n",
            "\n",
            "  temp_max:\n",
            "\n",
            "  temp_min:\n",
            "\n",
            "  temp_mean:\n",
            "\n",
            "  humidity_max:\n",
            "\n",
            "  humidity_min:\n",
            "\n",
            "  precipitation:\n",
            "    + COAST           × 17.8%\n",
            "    + EAST            × 2.2%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CREATING WEIGHTED WEATHER FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Weather columns to weight\n",
        "WEATHER_COLS = [\n",
        "    'temp_max',\n",
        "    'temp_min',\n",
        "    'temp_mean',\n",
        "    'humidity_max',\n",
        "    'humidity_min',\n",
        "    'precipitation',\n",
        "    'wind_speed_max'\n",
        "]\n",
        "\n",
        "# Start with dates from load data\n",
        "weighted_weather = load_df[['date']].copy().drop_duplicates()\n",
        "weighted_weather = weighted_weather.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nBase dataframe: {len(weighted_weather)} unique dates\")\n",
        "print(f\"Date range: {weighted_weather['date'].min()} to {weighted_weather['date'].max()}\")\n",
        "\n",
        "# Create weighted columns\n",
        "print(f\"\\nCreating weighted features:\")\n",
        "\n",
        "for col in WEATHER_COLS:\n",
        "    print(f\"\\n  {col}:\")\n",
        "\n",
        "    weighted_weather[f'weighted_{col}'] = 0.0\n",
        "\n",
        "    # Add contribution from each station\n",
        "    for station, weight in weights_normalized.items():\n",
        "        if station in weather_files:\n",
        "            df_station = weather_files[station]\n",
        "\n",
        "            # Find matching column (handle naming variations)\n",
        "            matching_cols = [c for c in df_station.columns if col.lower() in c.lower()]\n",
        "\n",
        "            if matching_cols:\n",
        "                actual_col = matching_cols[0]\n",
        "\n",
        "                # Merge on date\n",
        "                temp_merge = df_station[['date', actual_col]].copy()\n",
        "                temp_merge[actual_col] = pd.to_numeric(temp_merge[actual_col], errors='coerce')\n",
        "\n",
        "                # Merge into weighted_weather\n",
        "                weighted_weather = weighted_weather.merge(\n",
        "                    temp_merge.rename(columns={actual_col: f'{col}_{station}'}),\n",
        "                    on='date',\n",
        "                    how='left'\n",
        "                )\n",
        "\n",
        "                # Add weighted contribution\n",
        "                weighted_weather[f'weighted_{col}'] += (\n",
        "                    weighted_weather[f'{col}_{station}'] * weight\n",
        "                )\n",
        "\n",
        "                # Clean up temporary column\n",
        "                weighted_weather = weighted_weather.drop(columns=[f'{col}_{station}'])\n",
        "\n",
        "                print(f\"    + {station:15} × {weight:.1%}\")\n",
        "\n",
        "print(f\"\\n✓ Weighted features created!\")\n",
        "print(f\"\\nFinal shape: {weighted_weather.shape}\")\n",
        "print(f\"Columns: {weighted_weather.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtuYmYwfQ9hx"
      },
      "source": [
        "## Step 5: Verify and Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czl-DBjzQ9hx"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA QUALITY CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "missing = weighted_weather.isnull().sum()\n",
        "if missing.sum() > 0:\n",
        "    print(f\"\\n⚠️  Missing values found:\")\n",
        "    for col, count in missing[missing > 0].items():\n",
        "        pct = (count / len(weighted_weather)) * 100\n",
        "        print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
        "else:\n",
        "    print(f\"\\n✓ No missing values!\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample data:\")\n",
        "weighted_cols = [c for c in weighted_weather.columns if 'weighted' in c]\n",
        "print(weighted_weather[['date'] + weighted_cols[:3]].head(10))\n",
        "\n",
        "# Statistics\n",
        "print(f\"\\nWeighted feature statistics:\")\n",
        "for col in weighted_cols:\n",
        "    print(f\"\\n  {col}:\")\n",
        "    print(f\"    Mean: {weighted_weather[col].mean():.2f}\")\n",
        "    print(f\"    Min:  {weighted_weather[col].min():.2f}\")\n",
        "    print(f\"    Max:  {weighted_weather[col].max():.2f}\")\n",
        "    print(f\"    Std:  {weighted_weather[col].std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgIqb1VaQ9hx"
      },
      "source": [
        "## Step 6: Save Final File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ROzJ7doQ9hx"
      },
      "outputs": [],
      "source": [
        "OUTPUT_FILE = 'final_weighted_weather_features.csv'\n",
        "\n",
        "print(f\"\\nSaving to: {OUTPUT_FILE}\")\n",
        "\n",
        "# Keep only date and weighted columns\n",
        "output_cols = ['date'] + [c for c in weighted_weather.columns if 'weighted' in c]\n",
        "weighted_weather[output_cols].to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"✓ Saved!\")\n",
        "print(f\"\\nFile info:\")\n",
        "print(f\"  Rows: {len(weighted_weather):,}\")\n",
        "print(f\"  Columns: {len(output_cols)}\")\n",
        "print(f\"  Size: {Path(OUTPUT_FILE).stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(f\"✅ PREPROCESSING COMPLETE\")\n",
        "print(f\"=\"*80)\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"1. Upload {OUTPUT_FILE} to your working directory\")\n",
        "print(f\"2. In your training notebook, load it with:\")\n",
        "print(f\"   weather_df = pd.read_csv('{OUTPUT_FILE}')\")\n",
        "print(f\"3. Merge with load data on date\")\n",
        "print(f\"4. Train your models!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}